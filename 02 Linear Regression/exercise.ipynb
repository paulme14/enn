{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def embed_lecture_slides(path, width=\"100%\", height=\"600\"):\n",
    "    lecture_host = \"https://www.uni-muenster.de/AISystems/courses/drl/nn/\"\n",
    "    # lecture_host = \"http://localhost:8889/OnlineWebsite/\"\n",
    "    return IFrame(f\"{lecture_host}{path}\", width=width, height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Regression with Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_lecture_slides(\"02_Regression/02-regression-deck.html#/title-slide\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "z6vPe2jPxPgL",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"size\": 50, \"price\": 450, \"rooms\": 2, \"distance_to_center\": 5, \"district\": \"Gievenbeck\"},\n",
    "    {\"size\": 25, \"price\": 500, \"rooms\": 1, \"distance_to_center\": 3.5, \"district\": \"Sentrup\"},\n",
    "    {\"size\": 35, \"price\": 770, \"rooms\": 1, \"distance_to_center\": 3, \"district\": \"Wienburg\"},\n",
    "    {\"size\": 80, \"price\": 800, \"rooms\": 3, \"distance_to_center\": 8, \"district\": \"Nienberge\"},\n",
    "    {\"size\": 62, \"price\": 800, \"rooms\": 2, \"distance_to_center\": 5, \"district\": \"Coerde\"},\n",
    "    {\"size\": 70, \"price\": 820, \"rooms\": 3, \"distance_to_center\": 4.5, \"district\": \"Gievenbeck\"},\n",
    "    {\"size\": 19, \"price\": 440, \"rooms\": 1, \"distance_to_center\": 7, \"district\": \"Nienberge\"},\n",
    "    {\"size\": 73, \"price\": 1127, \"rooms\": 3.5, \"distance_to_center\": 5, \"district\": \"Wienburg\"},\n",
    "    {\"size\": 20, \"price\": 769, \"rooms\": 1, \"distance_to_center\": 1, \"district\": \"Zentrum\"},\n",
    "]\n",
    "\n",
    "# Extract regressors and regressands\n",
    "sizes = np.array([apt['size'] for apt in data], dtype=float)\n",
    "distances = np.array([apt['distance_to_center'] for apt in data], dtype=float)\n",
    "prices = np.array([apt['price'] for apt in data], dtype=float)\n",
    "rooms = np.array([apt['rooms'] for apt in data], dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize apartment price vs size\n",
    "%matplotlib inline\n",
    "\n",
    "plt.close()\n",
    "plt.scatter(x=sizes, y=prices)\n",
    "plt.xlabel('Size (in square meters)')\n",
    "plt.ylabel('Price (in EUR)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_lecture_slides(\"02_Regression/02-regression-deck.html#/basic-notation-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.column_stack((sizes, rooms))\n",
    "print(f\"{X.shape[0]} samples x {X.shape[1]} features:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Numpy and linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.ones(3)\n",
    "print(f\"{np.linalg.norm(x) ** 2:.1f}\")  # squared norm of x\n",
    "# print(x.T @ x)  # we expect the squared norm of x\n",
    "# print(x @ x.T)  # we expect a 3x3 matrix\n",
    "# print(x)  # we expect a column vector\n",
    "# print(x.T)  # we expect a row vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Why are we surprised?\n",
    "\n",
    "- In math, 1D vectors default to column orientation\n",
    "- Frame it like this: In math, 1D vector $x\\in\\mathbb{R}^m$ is implicitly treated as 2D vector $x\\in\\mathbb{R}^{m\\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- Numpy is general-purpose, not linear algebra\n",
    "- 1D numpy arrays are orientation-less (neither row nor column vector)\n",
    "- Many operations work as expected, though\n",
    "- Still, we sometimes need to explicitly implement the mathematical convention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Task: Explicit column vector in numpy**\n",
    "\n",
    "- Convert $x$ to a 2D numpy array that explicitly implements a column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "z = np.ones(3)\n",
    "x = z  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Task: Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    assert x.T @ x == 3\n",
    "    assert (x @ x.T).shape == (3, 3)\n",
    "    print(x)\n",
    "except:\n",
    "    print(\"Task not successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_lecture_slides(\"02_Regression/02-regression-deck.html#/error-functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Matrix notation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(w) &= \\frac{1}{m}\\sum^m_{i=1}(y^{(i)}-f(x^{(i)},w))^2\\\\\n",
    "&= \\frac{1}{m}\\lVert y-f(X,w)\\rVert_2^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Note:\n",
    "- as an optimization problem, we might omit the $\\frac{1}{m}$ factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Question:\n",
    "- Why do we take the square?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "PHuaI0eoxPgM",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred, verbose=True):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error between the actual and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: array-like, actual values (ground truth)\n",
    "    - y_pred: array-like, predicted values\n",
    "    \"\"\"\n",
    "    # Ensure inputs are NumPy arrays for element-wise operations\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate the squared differences\n",
    "    squared_differences = (y_true - y_pred) ** 2\n",
    "\n",
    "    # Return the mean of the squared differences\n",
    "    mse = np.mean(squared_differences)\n",
    "    if verbose:\n",
    "        print(f\"MSE: {mse: ,.1f}\")\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Linear Function Approximator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Notation\n",
    "For inputs $x^{(i)}\\in\\mathbb{R}^{d}$, linear regression optimizes parameters $b\\in\\mathbb{R}, w\\in\\mathbb{R}^{d}$ of a linear model\n",
    "$$\n",
    "f(x^{(i)}, \\{w, b\\}) := \\langle x^{(i)}, w \\rangle + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "or, in matrix notation\n",
    "$$\n",
    "f(X, \\{w, b\\}) := Xw + b\\cdot 1\n",
    "$$\n",
    "where $1\\in\\mathbb{R}^{m}$, $X\\in\\mathbb{R}^{m\\times d}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Intercept Convention\n",
    "By including the bias $b$ (aka intercept) as additional feature in the input\n",
    "$$\n",
    "X=[1 \\mid X]\\in \\mathbb{R}^{m\\times d+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we simply define the linear model as\n",
    "$$\n",
    "f(X, w) := Xw\n",
    "$$\n",
    "where now $w\\in\\mathbb{R}^{d+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Optimization Target\n",
    "Substituting $f$ in the optimization target (and omitting a scalar) we obtain\n",
    "$$\n",
    "\\underset{w}{\\mathrm{arg\\,min}}\\; \\lVert Xw - y\\rVert^2_2 \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Note (multivariate case):\n",
    "- for general multivariate linear regression, we just adapt $w, y$ to uppercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Experiment with fitting $w_0$ aka *intercept* and $w_1$ aka *slope* to model apartment price as a linear function of apartment size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "# Function to plot with adjustable slope and intercept\n",
    "def update_plot(ax, polynomial, quadratic=0, slope=1, intercept=0):\n",
    "    title = \"Interactive Linear Regression\"\n",
    "    legend = lambda _, b, c: f\"Line: y = {b:.2f}x + {c:.2f}\"\n",
    "    if polynomial:  # used further below\n",
    "        title = \"Interactive Polynomial Regression\"\n",
    "        legend = lambda a, b, c: f\"Line: y = {a:.2f}x^2 + {b:.2f}x + {c:.2f}\"\n",
    "\n",
    "    # Clear the axes but keep the figure\n",
    "    ax.clear()\n",
    "    \n",
    "    # Scatter plot of the actual data\n",
    "    ax.scatter(sizes, prices, color=\"C0\", label='Actual Prices')\n",
    "    \n",
    "    # Line equation: y = slope * x + intercept\n",
    "    predicted_prices = quadratic * sizes ** 2 + slope * sizes + intercept\n",
    "    \n",
    "    # Plot the fitted line with the current quadratic, slope and intercept\n",
    "    sizes_grid = np.linspace(sizes.min(), sizes.max(), 50)\n",
    "    predicted_prices_grid = quadratic * sizes_grid ** 2 + slope * sizes_grid + intercept\n",
    "    ax.plot(sizes_grid, predicted_prices_grid, color=\"C1\", label=legend(quadratic, slope, intercept), linestyle='--')\n",
    "\n",
    "    # Draw error bars: vertical lines between actual and predicted values - more efficient approach\n",
    "    error_lines = []\n",
    "    for i in range(len(sizes)):\n",
    "        error_lines.extend([[sizes[i], sizes[i]], [prices[i], predicted_prices[i]], [None, None]])\n",
    "    \n",
    "    # Plot all error lines at once (more efficient than individual plots)\n",
    "    x_coords = [error_lines[i] for i in range(0, len(error_lines), 3)]\n",
    "    y_coords = [error_lines[i] for i in range(1, len(error_lines), 3)]\n",
    "    \n",
    "    for x_line, y_line in zip(x_coords, y_coords):\n",
    "        ax.plot(x_line, y_line, color=\"C2\", linestyle='-', lw=1.5)\n",
    "\n",
    "    # Calculate and display the mean squared error (MSE)\n",
    "    mse = mean_squared_error(prices, predicted_prices, verbose=False)\n",
    "    ax.text(0.5, 0.9, f'Mean Squared Error: {mse:,.1f}', \n",
    "            horizontalalignment='center', verticalalignment='center', \n",
    "            transform=ax.transAxes, fontsize=12, color='black',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # Labels, title, and legend\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Size of Apartment (square meters)')\n",
    "    ax.set_ylabel('Price (EUR)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Redraw the canvas\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "%matplotlib widget\n",
    "plt.close()\n",
    "\n",
    "# Create persistent figure and axes for better performance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Interactive widgets for slope and intercept\n",
    "interact(update_plot, ax=fixed(ax), quadratic=fixed(0), polynomial=fixed(False),\n",
    "         slope=widgets.FloatSlider(min=-10, max=50, step=0.1, value=1), \n",
    "         intercept=widgets.FloatSlider(min=-1000, max=3000, step=50, value=0));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**Task: Model API**\n",
    "- In order to familiarize yourself with a typical model API, we implement a simple model\n",
    "- The simple model fits a straight line using only two data points\n",
    "- Complete the code in the `fit` and `predict` methods\n",
    "\n",
    "**Explanation:**\n",
    "1. `fit`: This method is supposed to train the model. Here, it calculates slope and intercept based on the first two data points.\n",
    "2. `predict`: This method is expected to provide predictions on new inputs based on its learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NaiveLinearModel:\n",
    "    \"\"\"\n",
    "    Naive model fits a straight line through the first two data points (x^(1), y^(1)) and (x^(2), y^(2))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.slope = None\n",
    "        self.intercept = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate perfect slope and intercept for straight line through (x^(1), y^(1)) and (x^(2), y^(2)).\n",
    "        We assume simple regression i.e. 1 feature dimension. \n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, 1)\n",
    "            The independent variable (e.g. apartment size) as explicit column vector(!)\n",
    "        y : numpy array, shape (n_samples)\n",
    "            The dependent variable (e.g. apartment price)\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # x1, y1 =  # TODO\n",
    "        # x2, y2 =  # TODO\n",
    "        # self.slope =  # TODO\n",
    "        # self.intercept = y1 - self.slope * x1\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Get model predictions on new data X.\n",
    "        Assumed to be called after `fit`.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, 1)\n",
    "            The independent variable (e.g. apartment size) as explicit column vector(!)\n",
    "\n",
    "        Returns:\n",
    "        predictions : numpy array, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        return #TODO\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot(X, y, predictions):\n",
    "        sorted_indices = np.argsort(X[:, 0])\n",
    "        plt.plot(X[sorted_indices], predictions[sorted_indices], color='red', label='Predicted Values')\n",
    "        plt.scatter(X, y, color='blue', label='Actual Values')\n",
    "        for x_val, y_true, y_hat in zip(X[:,0], y, predictions):\n",
    "            plt.plot([x_val, x_val], [y_true, y_hat], color='red', linestyle='--', linewidth=0.8)\n",
    "        plt.xlabel('Apartment size')\n",
    "        plt.ylabel('Apartment price')\n",
    "        plt.title(\"Simple linear regression\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Task: Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = sizes.reshape(-1, 1)  # explicit column vector\n",
    "y = prices\n",
    "\n",
    "try:\n",
    "    model = NaiveLinearModel()\n",
    "    model.fit(X, y)\n",
    "    assert model.slope is not None and model.intercept is not None, \"Model not fitted properly.\"\n",
    "    predictions = model.predict(X)\n",
    "    mean_squared_error(predictions, y)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    plt.close()\n",
    "    model.plot(X=X, y=prices, predictions=predictions)\n",
    "\n",
    "except:\n",
    "    print(\"Task not successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "0XXEy54DxPgT",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Analytic Solution\n",
    "The normal equation gives us an analytic solution to the minimization problem $\\underset{w}{\\mathrm{arg\\,min}}\\; \\lVert Xw - y\\rVert^2_2$\\\n",
    "where $X\\in\\mathbb{R}^{m\\times d}, w\\in\\mathbb{R}^d, y\\in\\mathbb{R}^m$:\n",
    "$$\n",
    "w = X^+ y\n",
    "$$\n",
    "with Moore-Penrose Pseudoinverse $X^+$. It holds \n",
    "$$\n",
    "X^+ = (X^\\intercal X)^{-1}X^\\intercal\n",
    "$$\n",
    "if $X^\\intercal X$ is invertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_lecture_slides(\"02_Regression/02-regression-deck.html#/minimize-mse-1/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # registers 3D projection\n",
    "from numpy.linalg import svd, matrix_rank, LinAlgError\n",
    "\n",
    "def plot_normal_equation(X, y, show_span=True, annotate=True):\n",
    "    \"\"\"\n",
    "    Visualize features and the normal-equation solution in R^3.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (3, p)\n",
    "        Feature matrix whose COLUMNS are the feature vectors (in R^3).\n",
    "    y : array-like, shape (3,)\n",
    "        Target vector (3 observations).\n",
    "    show_span : bool\n",
    "        If True, draw the 1D/2D span (line/plane) of the feature columns when rank < 3.\n",
    "    annotate : bool\n",
    "        If True, annotate feature points and theta values on the plot.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    y = np.asarray(y, dtype=float).reshape(3,)\n",
    "    \n",
    "    if X.shape[0] != 3:\n",
    "        raise ValueError(\"X must have 3 rows (3 observations). Got shape %s\" % (X.shape,))\n",
    "    if y.shape[0] != 3:\n",
    "        raise ValueError(\"y must be length 3 (one entry per observation).\")\n",
    "    \n",
    "    n_obs, p = X.shape\n",
    "    # Solve normal equation. Try direct solve first (if invertible), otherwise use pseudoinverse.\n",
    "    try:\n",
    "        XtX = X.T @ X\n",
    "        Xt_y = X.T @ y\n",
    "        theta = np.linalg.solve(XtX, Xt_y)  # (X^T X)^{-1} X^T y\n",
    "        method = \"normal-equation (solve)\"\n",
    "    except LinAlgError:\n",
    "        theta = np.linalg.pinv(X) @ y\n",
    "        method = \"pseudoinverse (pinv) fallback\"\n",
    "    \n",
    "    y_hat = X @ theta\n",
    "    resid = y - y_hat\n",
    "    resid_norm = np.linalg.norm(resid)\n",
    "    \n",
    "    # Prepare 3D figure\n",
    "    fig = plt.figure(figsize=(9, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_box_aspect((1,1,1))\n",
    "    \n",
    "    # Plot origin\n",
    "    origin = np.zeros(3)\n",
    "    ax.scatter(*origin, marker='o', s=40, label='origin')\n",
    "    \n",
    "    # Plot feature vectors (columns of X) as arrows from origin and points\n",
    "    for j in range(p):\n",
    "        v = X[:, j]\n",
    "        ax.quiver(0,0,0, v[0], v[1], v[2], length=1.0, arrow_length_ratio=0.08, linewidth=1.5)\n",
    "        ax.scatter(*v, s=80, marker='o')\n",
    "        if annotate:\n",
    "            ax.text(v[0], v[1], v[2], f'  f{j+1}', fontsize=10)\n",
    "    \n",
    "    # Plot y and y_hat\n",
    "    ax.scatter(*y, s=100, marker='X', label='y (target)')\n",
    "    ax.text(y[0], y[1], y[2], '  y', fontsize=11)\n",
    "    ax.scatter(*y_hat, s=100, marker='^', label=r'$\\hat y = X\\theta$ (projection)')\n",
    "    ax.text(y_hat[0], y_hat[1], y_hat[2], '  y_hat', fontsize=11)\n",
    "    \n",
    "    # Draw line from y to y_hat (error vector)\n",
    "    ax.plot([y[0], y_hat[0]], [y[1], y_hat[1]], [y[2], y_hat[2]],\n",
    "            linestyle='--', linewidth=1.5, label='residual (y - y_hat)')\n",
    "    # Also plot residual as an arrow from y_hat to y\n",
    "    ax.quiver(y_hat[0], y_hat[1], y_hat[2], resid[0], resid[1], resid[2],\n",
    "              length=1.0, arrow_length_ratio=0.1, linestyle='dotted')\n",
    "    \n",
    "    # Optionally plot the span (line or plane) spanned by the feature columns\n",
    "    if show_span:\n",
    "        # Use SVD to get orthonormal basis for column space (left singular vectors)\n",
    "        U, S, Vt = svd(X, full_matrices=True)\n",
    "        r = matrix_rank(X)\n",
    "        if r == 1:\n",
    "            u1 = U[:, 0]\n",
    "            # plot a line along u1 spanning roughly the magnitude of features and y\n",
    "            mags = np.hstack([np.linalg.norm(X, axis=0), np.linalg.norm(y)])\n",
    "            tmax = mags.max() * 1.2 if mags.size else 1.0\n",
    "            t = np.linspace(-tmax, tmax, 50)\n",
    "            line_pts = np.outer(u1, t)\n",
    "            ax.plot(line_pts[0, :], line_pts[1, :], line_pts[2, :], alpha=0.25, linewidth=3, label='span(feature) line')\n",
    "        elif r == 2:\n",
    "            # plane spanned by U[:,0] and U[:,1]\n",
    "            u1 = U[:, 0]\n",
    "            u2 = U[:, 1]\n",
    "            # choose coefficient ranges based on existing vectors' coordinates in the basis\n",
    "            # project existing vectors onto the plane to determine scale\n",
    "            coords = np.linalg.lstsq(np.column_stack([u1, u2]), np.column_stack([X, y]), rcond=None)[0]\n",
    "            max_val = np.abs(coords).max() if coords.size else 1.0\n",
    "            a = np.linspace(-max_val*1.2, max_val*1.2, 20)\n",
    "            b = np.linspace(-max_val*1.2, max_val*1.2, 20)\n",
    "            A, B = np.meshgrid(a, b)\n",
    "            plane_pts = (u1[:, None, None] * A[None, :, :]) + (u2[:, None, None] * B[None, :, :])\n",
    "            ax.plot_surface(plane_pts[0], plane_pts[1], plane_pts[2], alpha=0.25, rstride=1, cstride=1, shade=False)\n",
    "            ax.plot([], [], [], alpha=0.25, linewidth=3, label='span(feature) plane')  # dummy for legend\n",
    "        else:\n",
    "            # rank 3: span is full R^3, no need to draw\n",
    "            pass\n",
    "    \n",
    "    # Axis labels: each axis corresponds to that observation index\n",
    "    ax.set_xlabel('x_1 (obs 1)', fontsize=11)\n",
    "    ax.set_ylabel('x_2 (obs 2)', fontsize=11)\n",
    "    ax.set_zlabel('x_3 (obs 3)', fontsize=11)\n",
    "    \n",
    "    # Annotate theta and fit stats\n",
    "    theta_text = \"θ = [\" + \", \".join(f\"{t:.3g}\" for t in theta) + \"]\"\n",
    "    stats_text = f\"method: {method}\\nresidual norm ||y - ŷ|| = {resid_norm:.4g}\"\n",
    "    ax.text2D(0.02, 0.95, theta_text, transform=ax.transAxes, fontsize=10)\n",
    "    ax.text2D(0.02, 0.90, stats_text, transform=ax.transAxes, fontsize=10)\n",
    "    \n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1.02, 1.0))\n",
    "    ax.set_title(\"Features (columns of X), target y, and projection ŷ via the normal equation\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "X = np.column_stack((np.ones(len(sizes)), sizes))[:3]\n",
    "y = prices[:3]\n",
    "\n",
    "# Example 2: single feature -> line visible\n",
    "# X = np.array([[1.0], [2.0], [3.0]])\n",
    "# y = np.array([0.5, 1.0, 1.2])\n",
    "\n",
    "%matplotlib widget\n",
    "plt.close()\n",
    "plot_normal_equation(X, y, show_span=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_lecture_slides(\"02_Regression/02-regression-deck.html#/exercise-tasks-week-2---linear-regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "d0rNpfdNxPgT",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AnalyticLinearModel:\n",
    "    \"\"\"\n",
    "    A linear regression model that directly calculates the optimal weights\n",
    "    using the normal equation (closed-form solution) for multiple input dimensions.\n",
    "    This model does not provide an intercept term by itself, but instead expects\n",
    "    the input X to include a bias column as first column (all 1s).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weights = None  # Weights vector (including bias weight)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, n_features)\n",
    "            The independent variables with an additional bias column (first column is all 1s).\n",
    "        y : numpy array, shape (n_samples,)\n",
    "            The dependent variable (e.g., price of apartments).\n",
    "        \"\"\"\n",
    "        self.weights = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, n_features)\n",
    "            New data (independent variables with bias term as the first column).\n",
    "\n",
    "        Returns:\n",
    "        predictions : numpy array, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        return X @ self.weights\n",
    "    \n",
    "    def fit_and_plot(self, X, y, title=\"Linear Regression\"):\n",
    "        \"\"\"\n",
    "        Convenience method to both fit and plot.\n",
    "\n",
    "        Parameters:\n",
    "        X : numpy array, shape (n_samples, n_features).\n",
    "            n_features is assumed to be 1, 2 or 3.\n",
    "            If n_features > 1, bias is assumed to be included as first column.\n",
    "            Bias axis is omitted in plot.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        predictions = self.predict(X)\n",
    "        mean_squared_error(predictions, y)\n",
    "        if X.shape[1] > 1:\n",
    "            # assume first column is bias term (all 1s) which we do not plot as a proper feature\n",
    "            X = X[:, 1:]\n",
    "        assert X.shape[1] in [1, 2], \"Can only plot 1D or 2D data (excluding intercept)\"\n",
    "        if X.shape[1] == 1:\n",
    "            self.plot_1d(X, y, predictions, title=title)\n",
    "        elif X.shape[1] == 2:\n",
    "            x1_grid = np.linspace(X[:,0].min(), X[:,0].max(), 50)\n",
    "            x2_grid = np.linspace(X[:,1].min(), X[:,1].max(), 50)\n",
    "            x1_mesh, x2_mesh = np.meshgrid(x1_grid, x2_grid)\n",
    "            X_grid = np.c_[x1_mesh.ravel(), x2_mesh.ravel()]\n",
    "            if self.weights.shape[0] > X.shape[1]:  # fitted with intercept\n",
    "                X_grid = np.column_stack((np.ones(X_grid.shape[0]), X_grid))\n",
    "            predictions_mesh = self.predict(X_grid).reshape(x1_mesh.shape)\n",
    "            self.plot_2d(X, y, predictions, x1_mesh, x2_mesh, predictions_mesh, title=title)\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_1d(X, y, predictions, title, x1_mesh=None, predictions_mesh=None):\n",
    "        if x1_mesh is None or predictions_mesh is None:\n",
    "            x1_mesh = X\n",
    "            predictions_mesh = predictions\n",
    "        sorted_indices = np.argsort(x1_mesh[:, 0])\n",
    "        plt.plot(x1_mesh[sorted_indices], predictions_mesh[sorted_indices], color='red', label='Predicted Values')\n",
    "        plt.scatter(X, y, color='blue', label='Actual Values')\n",
    "        for x_val, y_true, y_hat in zip(X[:,0], y, predictions):\n",
    "            plt.plot([x_val, x_val], [y_true, y_hat], color='red', linestyle='--', linewidth=0.8)\n",
    "        plt.xlabel('X values')\n",
    "        plt.ylabel('y values')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_2d(X, y, predictions, x1_mesh, x2_mesh, predictions_mesh, title):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.plot_surface(x1_mesh, x2_mesh, predictions_mesh, color='red', alpha=0.5)\n",
    "        ax.scatter(X[:,0], X[:,1], y, color='blue')\n",
    "        ax.scatter(X[:,0], X[:,1], predictions, color='orange')\n",
    "        for x_val1, x_val2, y_true, y_hat in zip(X[:,0], X[:,1], y, predictions):\n",
    "            ax.plot([x_val1, x_val1], [x_val2, x_val2], [y_true, y_hat], color='red', linestyle='--', linewidth=0.8)\n",
    "        ax.set_xlabel('Feature 1')\n",
    "        ax.set_ylabel('Feature 2')\n",
    "        ax.set_zlabel('Target')\n",
    "        plt.title(title)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "t-14OSyTxPgT",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple Regression w/o intercept\n",
    "%matplotlib inline\n",
    "plt.close()\n",
    "X = np.column_stack((sizes, ))\n",
    "AnalyticLinearModel().fit_and_plot(X, y=prices, title=\"Simple Linear Regression w/o intercept: Size vs Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple Regression\n",
    "%matplotlib inline\n",
    "plt.close()\n",
    "X = np.column_stack((np.ones(sizes.shape[0]), sizes))\n",
    "AnalyticLinearModel().fit_and_plot(X, y=prices, title=\"Simple Linear Regression: Size vs Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Multiple Regression\n",
    "%matplotlib widget\n",
    "plt.close()\n",
    "X = np.column_stack((np.ones(sizes.shape[0]), sizes, distances))\n",
    "AnalyticLinearModel().fit_and_plot(X, y=prices, title=\"Multiple Linear Regression: Size & Distance vs Price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### 3rd Party Implementation - sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Linear Regression with sklearn**\n",
    "- Complete the code\n",
    "- Use an sklearn model predict `prices` from `sizes` via linear regression with intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = None  # TODO\n",
    "sklearn_model = None  # TODO\n",
    "sklearn_predictions = None  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    mean_squared_error(prices, sklearn_predictions)\n",
    "\n",
    "    %matplotlib inline\n",
    "    plt.close()\n",
    "    AnalyticLinearModel.plot_1d(X=X[:, 1:], y=prices, predictions=sklearn_predictions, title=\"sklearn - Simple Linear Regression: Size vs Price\")\n",
    "\n",
    "except:\n",
    "    print(\"Task not successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Visualizing the Error Surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.column_stack((np.ones(sizes.shape), sizes))\n",
    "errors = []\n",
    "model = AnalyticLinearModel()\n",
    "model.fit(X, prices)\n",
    "\n",
    "precision = 50\n",
    "slope_grid = np.linspace(model.weights[1] - 4, model.weights[1] + 4, precision)\n",
    "intercept_grid = np.linspace(model.weights[0] - 200, model.weights[0] + 200, precision)\n",
    "slope_mesh, intercept_mesh = np.meshgrid(slope_grid, intercept_grid)\n",
    "mesh = np.c_[slope_mesh.ravel(), intercept_mesh.ravel()]\n",
    "\n",
    "for slope, intercept in mesh:\n",
    "    model.weights = np.array([intercept, slope])\n",
    "    predictions = model.predict(X)\n",
    "    errors.append(mean_squared_error(prices, predictions, verbose=False))\n",
    "\n",
    "%matplotlib widget\n",
    "plt.close()\n",
    "error_mesh = np.clip(np.array(errors).reshape(slope_mesh.shape), 0, 35_000)  # clip for better visualization\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(slope_mesh, intercept_mesh, error_mesh, alpha=0.5)\n",
    "ax.set_xlabel(\"Slope\")\n",
    "ax.set_ylabel(\"Intercept\")\n",
    "ax.set_zlabel(\"MSE\")\n",
    "plt.title(\"Error Surface\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Outlook: Polynomial Regression\n",
    "### Motivation\n",
    "For *simple regression* $x, 1\\in\\mathbb{R}^{m}$: Instead of solving $\\underset{w}{\\mathrm{arg\\,min}}\\; \\lVert w_1x + w_0 1 - y\\rVert^2_2$ i.e.\n",
    "$$\n",
    "\\underset{w}{\\mathrm{arg\\,min}}\\; \\lVert \\sum^1_{k=0}w_kx^k - y\\rVert^2_2\n",
    "$$\n",
    "where $\\cdot^k$ is applied element-wise, we extend the model class to polynomials of degree $d$ and solve\n",
    "$$\n",
    "\\underset{w}{\\mathrm{arg\\,min}}\\; \\lVert \\sum^d_{k=0}w_kx^k - y\\rVert^2_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Note:\n",
    "- this can be extended to the multivariate case\n",
    "- by including the polynomial features directly in the input features $X$ (just like we did with the intercept) we stick to the same optimization target\n",
    "\n",
    "$$\n",
    "\\underset{w}{\\mathrm{arg\\,min}}\\; \\lVert Xw - y\\rVert^2_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "# Function to plot with adjustable slope and intercept\n",
    "def update_plot(ax, polynomial, quadratic=0, slope=1, intercept=0):\n",
    "    title = \"Interactive Linear Regression\"\n",
    "    legend = lambda _, b, c: f\"Line: y = {b:.2f}x + {c:.2f}\"\n",
    "    if polynomial:  # used further below\n",
    "        title = \"Interactive Polynomial Regression\"\n",
    "        legend = lambda a, b, c: f\"Line: y = {a:.2f}x^2 + {b:.2f}x + {c:.2f}\"\n",
    "\n",
    "    # Clear the axes but keep the figure\n",
    "    ax.clear()\n",
    "    \n",
    "    # Scatter plot of the actual data\n",
    "    ax.scatter(sizes, prices, color=\"C0\", label='Actual Prices')\n",
    "    \n",
    "    # Line equation: y = slope * x + intercept\n",
    "    predicted_prices = quadratic * sizes ** 2 + slope * sizes + intercept\n",
    "    \n",
    "    # Plot the fitted line with the current quadratic, slope and intercept\n",
    "    sizes_grid = np.linspace(sizes.min(), sizes.max(), 50)\n",
    "    predicted_prices_grid = quadratic * sizes_grid ** 2 + slope * sizes_grid + intercept\n",
    "    ax.plot(sizes_grid, predicted_prices_grid, color=\"C1\", label=legend(quadratic, slope, intercept), linestyle='--')\n",
    "\n",
    "    # Draw error bars: vertical lines between actual and predicted values - more efficient approach\n",
    "    error_lines = []\n",
    "    for i in range(len(sizes)):\n",
    "        error_lines.extend([[sizes[i], sizes[i]], [prices[i], predicted_prices[i]], [None, None]])\n",
    "    \n",
    "    # Plot all error lines at once (more efficient than individual plots)\n",
    "    x_coords = [error_lines[i] for i in range(0, len(error_lines), 3)]\n",
    "    y_coords = [error_lines[i] for i in range(1, len(error_lines), 3)]\n",
    "    \n",
    "    for x_line, y_line in zip(x_coords, y_coords):\n",
    "        ax.plot(x_line, y_line, color=\"C2\", linestyle='-', lw=1.5)\n",
    "\n",
    "    # Calculate and display the mean squared error (MSE)\n",
    "    mse = mean_squared_error(prices, predicted_prices, verbose=False)\n",
    "    ax.text(0.5, 0.9, f'Mean Squared Error: {mse:,.1f}', \n",
    "            horizontalalignment='center', verticalalignment='center', \n",
    "            transform=ax.transAxes, fontsize=12, color='black',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # Labels, title, and legend\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Size of Apartment (square meters)')\n",
    "    ax.set_ylabel('Price (EUR)')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Redraw the canvas\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "%matplotlib widget\n",
    "plt.close()\n",
    "\n",
    "# Create persistent figure and axes for better performance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Interactive widgets for slope and intercept\n",
    "interact(update_plot, ax=fixed(ax), polynomial=fixed(True),\n",
    "         quadratic=widgets.FloatSlider(min=-1, max=1, step=0.01, value=0),\n",
    "         slope=widgets.FloatSlider(min=-80, max=80, step=0.1, value=1), \n",
    "         intercept=widgets.FloatSlider(min=-1000, max=3000, step=50, value=0));\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ais-nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "rise": {
   "center": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
